{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3fd7d2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b16c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c006424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_list(filenames):\n",
    "    result_ls = set()\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.strip().split(',')\n",
    "                if len(words) == 1:\n",
    "                    result_ls.add(words[0])\n",
    "                else:\n",
    "                    result_ls.add(tuple(words))\n",
    "    return result_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6692ec",
   "metadata": {},
   "source": [
    "# Parse access list and block list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e654db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = read_words_list([\"whitelist/word1_w.txt\", \"whitelist/word2_w.txt\", \"whitelist/word3_w.txt\", \"whitelist/word4_w.txt\"])\n",
    "bl = read_words_list([\"blacklist/word1_b.txt\", \"blacklist/word2_b.txt\", \"blacklist/word3_b.txt\", \"blacklist/word4_b.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9bc64",
   "metadata": {},
   "source": [
    "# Parse incoming whitepapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c861074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_whitepapers(filename, stopwords):\n",
    "    directory = \"../whitepapers/top20_whitepapers/\"\n",
    "    words_list = []\n",
    "    words_context_dict = {}\n",
    "    lines = []\n",
    "    # context_tuples_ref = []\n",
    "    word_idx = 0\n",
    "    context_idx = 0\n",
    "    for entry in os.scandir(directory):\n",
    "        if (entry.path.endswith(filename) and entry.is_file()):\n",
    "            with open(entry.path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    # context_tuples_ref += [line]\n",
    "                    temp_words = extract_and_clean(line, stopwords)\n",
    "                    words_list.extend(temp_words)\n",
    "                    for i in range(len(temp_words)):\n",
    "                        words_context_dict[word_idx] = context_idx\n",
    "                        word_idx += 1\n",
    "                    context_idx += 1\n",
    "                    \n",
    "                    if len(line) > 100:\n",
    "                        lines.append(line)\n",
    "                        \n",
    "    return words_list, words_context_dict, lines #, context_tuples_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df55051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(line, stopwords):\n",
    "    # words = [x.strip() for x in re.split(',| |\\. |\\: ', line) if x]\n",
    "    # words = map(str.lower, words)\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', line.lower()))\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    words = [lemmatizer.lemmatize(w, pos='s') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='n') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='a') for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea4dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_word_bank():\n",
    "    word_bank = {}\n",
    "    context_ref = {}\n",
    "    lines_bank = {}\n",
    "    for i in filenames:\n",
    "        word_bank[i], context_ref[i], lines_bank[i] = read_whitepapers(i, stopwords_set)\n",
    "    return word_bank, context_ref, lines_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2196603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(words, context_ref):\n",
    "    appearance_dict = {}\n",
    "    for i in range(len(words)):\n",
    "        appearance_dict.setdefault(words[i],[]).append(context_ref[i])\n",
    "    return list(set(words)), appearance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a2b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wl_bl_words(words, context_ref, wl, bl):\n",
    "    unfiltered_words = []\n",
    "    filtered_words = []\n",
    "    filtered_context = {}\n",
    "    for w in words:\n",
    "        unfiltered_words.append(w)\n",
    "        if w in wl and w not in bl:\n",
    "            filtered_words.append(w)\n",
    "            filtered_context[w] = context_ref[w]\n",
    "    return unfiltered_words, filtered_words, filtered_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a894824",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "167fdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['Algorand.txt', 'Avalanche.txt', 'Binance.txt', 'Bitcoin.txt', 'Cardano.txt', 'Chainlink.txt',\n",
    "            'Crypto_com.txt', 'Ethereum.txt', 'FTX_token.txt', 'PolkaDot.txt', 'Polygon.txt', 'Ripple.txt', \n",
    "            'Solana.txt', 'Terra.txt', 'Tether.txt', 'Tron.txt', 'Uniswap.txt', 'Wrapped.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a970e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bank, context_ref, lines_bank = creat_word_bank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0f7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lookup_dict(lookup_dict, words, paper_name, appearance_dict):\n",
    "    for w in words:\n",
    "        lookup_dict.setdefault(w,[]).append((paper_name, appearance_dict[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a6c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35660aaa",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b91fe",
   "metadata": {},
   "source": [
    "## Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc08ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words = []\n",
    "agg_wl_words = []\n",
    "\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words, deduped_appearance_dict = dedupe(words_ls, words_context_dict)\n",
    "    raw_words, wl_words, wl_appearance_dict = filter_wl_bl_words(deduped_words, deduped_appearance_dict, wl, bl)\n",
    "    agg_words.extend(raw_words)\n",
    "    agg_wl_words.extend(wl_words)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words, coin, wl_appearance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0078fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28871\n"
     ]
    }
   ],
   "source": [
    "# The number of raw single word extracted from 20 whitepapers\n",
    "print(len(agg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595e18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered single word from 20 whitepapers\n",
    "print(len(agg_wl_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10936a8c",
   "metadata": {},
   "source": [
    "## Two Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b6fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_2gram = []\n",
    "agg_wl_words_2 = []\n",
    "stats_2 = {}\n",
    "stats_wl_2 = {}\n",
    "\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_2, deduped_appearance_dict_2 = dedupe(list(ngrams(words_ls, 2)), words_context_dict)\n",
    "    raw_words_2gram, wl_words_2, wl_appearance_dict_2 = filter_wl_bl_words(deduped_words_2, deduped_appearance_dict_2, wl, bl)\n",
    "    stats_2[coin] = len(raw_words_2gram)\n",
    "    stats_wl_2[coin] = len(wl_words_2)\n",
    "    agg_words_2gram.extend(raw_words_2gram)\n",
    "    agg_wl_words_2.extend(wl_words_2)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_2, coin, wl_appearance_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edea44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95618\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 2-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_2gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dacc5a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 2-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27fee813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 1372,\n",
       " 'Avalanche.txt': 2999,\n",
       " 'Binance.txt': 1809,\n",
       " 'Bitcoin.txt': 1793,\n",
       " 'Cardano.txt': 15418,\n",
       " 'Chainlink.txt': 26506,\n",
       " 'Crypto_com.txt': 6418,\n",
       " 'Ethereum.txt': 6807,\n",
       " 'FTX_token.txt': 1677,\n",
       " 'PolkaDot.txt': 8469,\n",
       " 'Polygon.txt': 4088,\n",
       " 'Ripple.txt': 1428,\n",
       " 'Solana.txt': 3551,\n",
       " 'Terra.txt': 2370,\n",
       " 'Tether.txt': 2864,\n",
       " 'Tron.txt': 4162,\n",
       " 'Uniswap.txt': 2006,\n",
       " 'Wrapped.txt': 1881}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "015067d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 70,\n",
       " 'Avalanche.txt': 250,\n",
       " 'Binance.txt': 114,\n",
       " 'Bitcoin.txt': 169,\n",
       " 'Cardano.txt': 483,\n",
       " 'Chainlink.txt': 964,\n",
       " 'Crypto_com.txt': 367,\n",
       " 'Ethereum.txt': 476,\n",
       " 'FTX_token.txt': 77,\n",
       " 'PolkaDot.txt': 330,\n",
       " 'Polygon.txt': 392,\n",
       " 'Ripple.txt': 45,\n",
       " 'Solana.txt': 206,\n",
       " 'Terra.txt': 139,\n",
       " 'Tether.txt': 162,\n",
       " 'Tron.txt': 299,\n",
       " 'Uniswap.txt': 106,\n",
       " 'Wrapped.txt': 186}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_wl_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a60f34",
   "metadata": {},
   "source": [
    "## Three Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17831706",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_3gram = []\n",
    "agg_wl_words_3 = []\n",
    "stats_3 = {}\n",
    "stats_wl_3 = {}\n",
    "\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_3, deduped_appearance_dict_3 = dedupe(list(ngrams(words_ls, 3)), words_context_dict)\n",
    "    raw_words_3gram, wl_words_3, wl_appearance_dict_3 = filter_wl_bl_words(deduped_words_3, deduped_appearance_dict_3, wl, bl)\n",
    "    stats_3[coin] = len(raw_words_3gram)\n",
    "    stats_wl_3[coin] = len(wl_words_3)\n",
    "    agg_words_3gram.extend(raw_words_3gram)\n",
    "    agg_wl_words_3.extend(wl_words_3)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_3, coin, wl_appearance_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad1712e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111328\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 3-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_3gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56b7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 3-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdf77df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 1783,\n",
       " 'Avalanche.txt': 3255,\n",
       " 'Binance.txt': 1966,\n",
       " 'Bitcoin.txt': 1972,\n",
       " 'Cardano.txt': 19204,\n",
       " 'Chainlink.txt': 31599,\n",
       " 'Crypto_com.txt': 7405,\n",
       " 'Ethereum.txt': 7695,\n",
       " 'FTX_token.txt': 1870,\n",
       " 'PolkaDot.txt': 8983,\n",
       " 'Polygon.txt': 4780,\n",
       " 'Ripple.txt': 1486,\n",
       " 'Solana.txt': 4078,\n",
       " 'Terra.txt': 2645,\n",
       " 'Tether.txt': 3261,\n",
       " 'Tron.txt': 4941,\n",
       " 'Uniswap.txt': 2218,\n",
       " 'Wrapped.txt': 2187}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921b822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 11,\n",
       " 'Avalanche.txt': 25,\n",
       " 'Binance.txt': 9,\n",
       " 'Bitcoin.txt': 29,\n",
       " 'Cardano.txt': 56,\n",
       " 'Chainlink.txt': 92,\n",
       " 'Crypto_com.txt': 43,\n",
       " 'Ethereum.txt': 45,\n",
       " 'FTX_token.txt': 4,\n",
       " 'PolkaDot.txt': 17,\n",
       " 'Polygon.txt': 48,\n",
       " 'Ripple.txt': 0,\n",
       " 'Solana.txt': 14,\n",
       " 'Terra.txt': 6,\n",
       " 'Tether.txt': 14,\n",
       " 'Tron.txt': 53,\n",
       " 'Uniswap.txt': 4,\n",
       " 'Wrapped.txt': 21}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_wl_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d62de",
   "metadata": {},
   "source": [
    "## Four Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c5b562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_4gram = []\n",
    "agg_wl_words_4 = []\n",
    "stats_4 = {}\n",
    "stats_wl_4 = {}\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_4, deduped_appearance_dict_4 = dedupe(list(ngrams(words_ls, 4)), words_context_dict)\n",
    "    raw_words_4gram, wl_words_4, wl_appearance_dict_4 = filter_wl_bl_words(deduped_words_4, deduped_appearance_dict_4, wl, bl)\n",
    "    stats_4[coin] = len(raw_words_4gram)\n",
    "    stats_wl_4[coin] = len(wl_words_4)\n",
    "    agg_words_4gram.extend(raw_words_4gram)\n",
    "    agg_wl_words_4.extend(wl_words_4)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_4, coin, wl_appearance_dict_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b1a7606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114807\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 4-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_4gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed44f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 4-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f73794d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 1991,\n",
       " 'Avalanche.txt': 3295,\n",
       " 'Binance.txt': 1984,\n",
       " 'Bitcoin.txt': 2016,\n",
       " 'Cardano.txt': 20184,\n",
       " 'Chainlink.txt': 32486,\n",
       " 'Crypto_com.txt': 7674,\n",
       " 'Ethereum.txt': 7831,\n",
       " 'FTX_token.txt': 1931,\n",
       " 'PolkaDot.txt': 9039,\n",
       " 'Polygon.txt': 4979,\n",
       " 'Ripple.txt': 1495,\n",
       " 'Solana.txt': 4183,\n",
       " 'Terra.txt': 2720,\n",
       " 'Tether.txt': 3356,\n",
       " 'Tron.txt': 5143,\n",
       " 'Uniswap.txt': 2255,\n",
       " 'Wrapped.txt': 2245}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8def5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': 5,\n",
       " 'Avalanche.txt': 6,\n",
       " 'Binance.txt': 0,\n",
       " 'Bitcoin.txt': 7,\n",
       " 'Cardano.txt': 21,\n",
       " 'Chainlink.txt': 22,\n",
       " 'Crypto_com.txt': 15,\n",
       " 'Ethereum.txt': 1,\n",
       " 'FTX_token.txt': 0,\n",
       " 'PolkaDot.txt': 3,\n",
       " 'Polygon.txt': 10,\n",
       " 'Ripple.txt': 0,\n",
       " 'Solana.txt': 0,\n",
       " 'Terra.txt': 0,\n",
       " 'Tether.txt': 3,\n",
       " 'Tron.txt': 3,\n",
       " 'Uniswap.txt': 0,\n",
       " 'Wrapped.txt': 2}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_wl_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae855c",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "Steps: \n",
    "1. Sample 8 paragraphs from 3 whitepapers\n",
    "2. Manually extract technical terms, named \"X\"\n",
    "3. Run through models to extract lists \"Y\"\n",
    "4. Calculate accuracy, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a694d3",
   "metadata": {},
   "source": [
    "### Sample\n",
    "Randomly chose 8 paragraphs from 5 whitepapers  \n",
    "Store them for manually labelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70627494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7f41633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paper = ['Avalanche.txt', 'Algorand.txt', 'Bitcoin.txt', 'Ethereum.txt', 'Chainlink.txt']\n",
    "sample = {}\n",
    "for i in sample_paper:\n",
    "    sample[i] = random.sample(lines_bank[i], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d4e5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sample)\n",
    "df.to_csv('manual/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e177d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {}\n",
    "for i in sample_paper:\n",
    "    test_lines = []\n",
    "    \n",
    "    for line in sample[i]:\n",
    "        test_words = extract_and_clean(line, stopwords_set)\n",
    "        test_lines.extend(test_words)\n",
    "    test_set[i] = test_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9e047",
   "metadata": {},
   "source": [
    "### Test bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9a286f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_words_2gram = []\n",
    "agg_test_wl_words_2 = []\n",
    "test_2grams = {}\n",
    "for coin in sample_paper:\n",
    "    test_ls = test_set[coin]\n",
    "    test_context_dict = context_ref[coin]\n",
    "    test_deduped_words_2, test_deduped_appearance_dict_2 = dedupe(list(ngrams(test_ls, 2)), test_context_dict)\n",
    "    test_words_2gram, test_wl_words_2, test_wl_appearance_dict_2 = filter_wl_bl_words(test_deduped_words_2, test_deduped_appearance_dict_2, wl, bl)\n",
    "    agg_test_words_2gram.extend(test_words_2gram)\n",
    "    agg_test_wl_words_2.extend(test_wl_words_2)\n",
    "    test_2grams[coin] = test_wl_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0544242e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_words_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f908d06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_wl_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bdae9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('global', 'network'),\n",
       " ('sharding', 'process'),\n",
       " ('proofofstake', 'protocol'),\n",
       " ('resource', 'order'),\n",
       " ('native', 'token'),\n",
       " ('view', 'network'),\n",
       " ('stable', 'view'),\n",
       " ('support', 'global'),\n",
       " ('veriﬁable', 'random'),\n",
       " ('function', 'vrfs'),\n",
       " ('future', 'period'),\n",
       " ('random', 'function'),\n",
       " ('block', 'protocol'),\n",
       " ('network', 'partition'),\n",
       " ('special', 'transaction'),\n",
       " ('transaction', 'block'),\n",
       " ('value', 'transaction'),\n",
       " ('network', 'node'),\n",
       " ('incentive', 'value'),\n",
       " ('merkle', 'tree'),\n",
       " ('node', 'receive'),\n",
       " ('transaction', 'fee'),\n",
       " ('transaction', 'hash'),\n",
       " ('place', 'chain'),\n",
       " ('financial', 'institution'),\n",
       " ('input', 'value'),\n",
       " ('hash', 'merkle'),\n",
       " ('acm', 'conference'),\n",
       " ('block', 'hash'),\n",
       " ('sybil', 'attack'),\n",
       " ('satoshi', 'nakamoto'),\n",
       " ('assume', 'transaction'),\n",
       " ('merkle', 'tree'),\n",
       " ('source', 'randomness'),\n",
       " ('computational', 'resource'),\n",
       " ('full', 'node'),\n",
       " ('financial', 'instrument'),\n",
       " ('support', 'generalize'),\n",
       " ('oracle', 'functionality'),\n",
       " ('sus', 'protocol')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_test_wl_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04bf3b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': [('veriﬁable', 'random'),\n",
       "  ('function', 'vrfs'),\n",
       "  ('future', 'period'),\n",
       "  ('random', 'function'),\n",
       "  ('block', 'protocol'),\n",
       "  ('network', 'partition')],\n",
       " 'Avalanche.txt': [('global', 'network'),\n",
       "  ('sharding', 'process'),\n",
       "  ('proofofstake', 'protocol'),\n",
       "  ('resource', 'order'),\n",
       "  ('native', 'token'),\n",
       "  ('view', 'network'),\n",
       "  ('stable', 'view'),\n",
       "  ('support', 'global')],\n",
       " 'Bitcoin.txt': [('special', 'transaction'),\n",
       "  ('transaction', 'block'),\n",
       "  ('value', 'transaction'),\n",
       "  ('network', 'node'),\n",
       "  ('incentive', 'value'),\n",
       "  ('merkle', 'tree'),\n",
       "  ('node', 'receive'),\n",
       "  ('transaction', 'fee'),\n",
       "  ('transaction', 'hash'),\n",
       "  ('place', 'chain'),\n",
       "  ('financial', 'institution'),\n",
       "  ('input', 'value'),\n",
       "  ('hash', 'merkle'),\n",
       "  ('acm', 'conference'),\n",
       "  ('block', 'hash')],\n",
       " 'Chainlink.txt': [('support', 'generalize'),\n",
       "  ('oracle', 'functionality'),\n",
       "  ('sus', 'protocol')],\n",
       " 'Ethereum.txt': [('sybil', 'attack'),\n",
       "  ('satoshi', 'nakamoto'),\n",
       "  ('assume', 'transaction'),\n",
       "  ('merkle', 'tree'),\n",
       "  ('source', 'randomness'),\n",
       "  ('computational', 'resource'),\n",
       "  ('full', 'node'),\n",
       "  ('financial', 'instrument')]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d6ddf",
   "metadata": {},
   "source": [
    "### Test trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4de3c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_words_3gram = []\n",
    "agg_test_wl_words_3 = []\n",
    "test_3grams = {}\n",
    "\n",
    "for coin in sample_paper:\n",
    "    test_ls = test_set[coin]\n",
    "    test_context_dict = context_ref[coin]\n",
    "    test_deduped_words_3, test_deduped_appearance_dict_3 = dedupe(list(ngrams(test_ls, 3)), test_context_dict)\n",
    "    test_words_3gram, test_wl_words_3, test_wl_appearance_dict_3 = filter_wl_bl_words(test_deduped_words_3, test_deduped_appearance_dict_3, wl, bl)\n",
    "    agg_test_words_3gram.extend(test_words_3gram)\n",
    "    agg_test_wl_words_3.extend(test_wl_words_3)\n",
    "    test_3grams[coin] = test_wl_words_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f25bad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_words_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c8c54db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_wl_words_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0525dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('veriﬁable', 'random', 'function'),\n",
       " ('random', 'function', 'vrfs'),\n",
       " ('root', 'include', 'block'),\n",
       " ('transaction', 'hash', 'merkle')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_test_wl_words_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "992fc30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': [('veriﬁable', 'random', 'function'),\n",
       "  ('random', 'function', 'vrfs')],\n",
       " 'Avalanche.txt': [],\n",
       " 'Bitcoin.txt': [('root', 'include', 'block'),\n",
       "  ('transaction', 'hash', 'merkle')],\n",
       " 'Chainlink.txt': [],\n",
       " 'Ethereum.txt': []}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_3grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f62011",
   "metadata": {},
   "source": [
    "### Test four gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d97a2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_words_4gram = []\n",
    "agg_test_wl_words_4 = []\n",
    "test_4grams = {}\n",
    "\n",
    "for coin in sample_paper:\n",
    "    test_ls = test_set[coin]\n",
    "    test_context_dict = context_ref[coin]\n",
    "    test_deduped_words_4, test_deduped_appearance_dict_4 = dedupe(list(ngrams(test_ls, 4)), test_context_dict)\n",
    "    test_words_4gram, test_wl_words_4, test_wl_appearance_dict_4 = filter_wl_bl_words(test_deduped_words_4, test_deduped_appearance_dict_4, wl, bl)\n",
    "    agg_test_words_4gram.extend(test_words_4gram)\n",
    "    agg_test_wl_words_4.extend(test_wl_words_4)\n",
    "    test_4grams[coin] = test_wl_words_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1ae9e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_words_4gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d43a22a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agg_test_wl_words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc510c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('veriﬁable', 'random', 'function', 'vrfs')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_test_wl_words_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8218f16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algorand.txt': [('veriﬁable', 'random', 'function', 'vrfs')],\n",
       " 'Avalanche.txt': [],\n",
       " 'Bitcoin.txt': [],\n",
       " 'Chainlink.txt': [],\n",
       " 'Ethereum.txt': []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "684f80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive: model labeled correctly\n",
    "bigram_tp = 25\n",
    "trigram_tp = 2\n",
    "fourgram_tp = 1\n",
    "\n",
    "# False Positive: model labeled incorrectly\n",
    "bigram_fp = 15\n",
    "trigram_fp = 2\n",
    "fourgram_fp = 0\n",
    "\n",
    "# True Negative: model unlabeled correctly\n",
    "bigram_tn = 351\n",
    "trigram_tn = 385\n",
    "fourgram_tn = 383\n",
    "\n",
    "# False Negative: model unlabeled incorrectly\n",
    "bigram_fn = 37\n",
    "trigram_fn = 18\n",
    "fourgram_fn = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf35685",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy = TP / (TP + TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ecc021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bigram: 0.066\n",
      "Accuracy of Trigram: 0.005\n",
      "Accuracy of Fourgram: 0.003\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "bigram_acc = bigram_tp / (bigram_tp + bigram_tn)\n",
    "\n",
    "# Trigram\n",
    "trigram_acc = trigram_tp / (trigram_tp + trigram_tn)\n",
    "\n",
    "# Fourgram\n",
    "fourgram_acc = fourgram_tp / (fourgram_tp + fourgram_tn)\n",
    "\n",
    "print(\"Accuracy of Bigram: {:.3f}\".format(bigram_acc))\n",
    "\n",
    "print(\"Accuracy of Trigram: {:.3f}\".format(trigram_acc))\n",
    "\n",
    "print(\"Accuracy of Fourgram: {:.3f}\".format(fourgram_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ca450",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9212b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Bigram: 0.625\n",
      "Precision of Trigram: 0.500\n",
      "Precision of Fourgram: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "bigram_pre = bigram_tp / (bigram_tp + bigram_fp)\n",
    "\n",
    "# Trigram\n",
    "trigram_pre = trigram_tp / (trigram_tp + trigram_fp)\n",
    "\n",
    "# Fourgram\n",
    "fourgram_pre = fourgram_tp / (fourgram_tp + fourgram_fp)\n",
    "\n",
    "print(\"Precision of Bigram: {:.3f}\".format(bigram_pre))\n",
    "\n",
    "print(\"Precision of Trigram: {:.3f}\".format(trigram_pre))\n",
    "\n",
    "print(\"Precision of Fourgram: {:.3f}\".format(fourgram_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeed2c6",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "161fa0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of Bigram: 0.403\n",
      "Recall of Trigram: 0.100\n",
      "Recall of Fourgram: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "bigram_rec = bigram_tp / (bigram_tp + bigram_fn)\n",
    "\n",
    "# Trigram\n",
    "trigram_rec = trigram_tp / (trigram_tp + trigram_fn)\n",
    "\n",
    "# Fourgram\n",
    "fourgram_rec = fourgram_tp / (fourgram_tp + fourgram_fn)\n",
    "\n",
    "print(\"Recall of Bigram: {:.3f}\".format(bigram_rec))\n",
    "\n",
    "print(\"Recall of Trigram: {:.3f}\".format(trigram_rec))\n",
    "\n",
    "print(\"Recall of Fourgram: {:.3f}\".format(fourgram_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f0889",
   "metadata": {},
   "source": [
    "### F1\n",
    "F1 = 2 * Precision * Recall / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a9ed47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bigram: 0.490\n",
      "F1 score of Trigram: 0.167\n",
      "F1 score of Fourgram: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "bigram_f1 = 2 * bigram_pre * bigram_rec / (bigram_pre + bigram_rec)\n",
    "\n",
    "# Trigram\n",
    "trigram_f1 = 2 * trigram_pre * trigram_rec / (trigram_pre + trigram_rec)\n",
    "\n",
    "# Fourgram\n",
    "fourgram_f1 = 2 * fourgram_pre * fourgram_rec / (fourgram_pre + fourgram_rec)\n",
    "\n",
    "print(\"F1 score of Bigram: {:.3f}\".format(bigram_f1))\n",
    "\n",
    "print(\"F1 score of Trigram: {:.3f}\".format(trigram_f1))\n",
    "\n",
    "print(\"F1 score of Fourgram: {:.3f}\".format(fourgram_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
