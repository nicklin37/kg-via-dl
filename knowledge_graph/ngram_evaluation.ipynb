{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a02e5be",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d5aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_list(filenames):\n",
    "    result_ls = set()\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.strip().split(',')\n",
    "                if len(words) == 1:\n",
    "                    result_ls.add(words[0])\n",
    "                else:\n",
    "                    result_ls.add(tuple(words))\n",
    "    return result_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e3a00",
   "metadata": {},
   "source": [
    "# Parse access list and block list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3322fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = read_words_list([\"whitelist/word1_w.txt\", \"whitelist/word2_w.txt\", \"whitelist/word3_w.txt\", \"whitelist/word4_w.txt\"])\n",
    "bl = read_words_list([\"blacklist/word1_b.txt\", \"blacklist/word2_b.txt\", \"blacklist/word3_b.txt\", \"blacklist/word4_b.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5acb5",
   "metadata": {},
   "source": [
    "# Parse incoming whitepapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490346c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_whitepapers(filename, stopwords):\n",
    "    directory = \"../whitepapers/top20_whitepapers/\"\n",
    "    words_list = []\n",
    "    words_context_dict = {}\n",
    "    # context_tuples_ref = []\n",
    "    word_idx = 0\n",
    "    context_idx = 0\n",
    "    for entry in os.scandir(directory):\n",
    "        if (entry.path.endswith(filename) and entry.is_file()):\n",
    "            with open(entry.path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    # context_tuples_ref += [line]\n",
    "                    temp_words = extract_and_clean(line, stopwords)\n",
    "                    words_list.extend(temp_words)\n",
    "                    for i in range(len(temp_words)):\n",
    "                        words_context_dict[word_idx] = context_idx\n",
    "                        word_idx += 1\n",
    "                    context_idx += 1\n",
    "    return words_list, words_context_dict #, context_tuples_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d362e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(line, stopwords):\n",
    "    # words = [x.strip() for x in re.split(',| |\\. |\\: ', line) if x]\n",
    "    # words = map(str.lower, words)\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', line.lower()))\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    words = [lemmatizer.lemmatize(w, pos='s') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='n') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='a') for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e652ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_word_bank():\n",
    "    word_bank = {}\n",
    "    context_ref = {}\n",
    "    for i in filenames:\n",
    "        word_bank[i], context_ref[i] = read_whitepapers(i, stopwords_set)\n",
    "    return word_bank, context_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933ce295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(words, context_ref):\n",
    "    appearance_dict = {}\n",
    "    for i in range(len(words)):\n",
    "        appearance_dict.setdefault(words[i],[]).append(context_ref[i])\n",
    "    return list(set(words)), appearance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c65e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wl_bl_words(words, context_ref, wl, bl):\n",
    "    unfiltered_words = []\n",
    "    filtered_words = []\n",
    "    filtered_context = {}\n",
    "    for w in words:\n",
    "        unfiltered_words.append(w)\n",
    "        if w in wl and w not in bl:\n",
    "            filtered_words.append(w)\n",
    "            filtered_context[w] = context_ref[w]\n",
    "    return unfiltered_words, filtered_words, filtered_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd587811",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb989a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['Algorand.txt', 'Avalanche.txt', 'Binance.txt', 'Bitcoin.txt', 'Cardano.txt', 'Chainlink.txt',\n",
    "            'Crypto_com.txt', 'Ethereum.txt', 'FTX_token.txt', 'PolkaDot.txt', 'Polygon.txt', 'Ripple.txt', \n",
    "            'Solana.txt', 'Terra.txt', 'Tether.txt', 'Tron.txt', 'Uniswap.txt', 'Wrapped.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b829218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bank, context_ref = creat_word_bank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd45946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lookup_dict(lookup_dict, words, paper_name, appearance_dict):\n",
    "    for w in words:\n",
    "        lookup_dict.setdefault(w,[]).append((paper_name, appearance_dict[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58db15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29b6ce",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daaeeb",
   "metadata": {},
   "source": [
    "## Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cde7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words = []\n",
    "agg_wl_words = []\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words, deduped_appearance_dict = dedupe(words_ls, words_context_dict)\n",
    "    raw_words, wl_words, wl_appearance_dict = filter_wl_bl_words(deduped_words, deduped_appearance_dict, wl, bl)\n",
    "    agg_words.extend(raw_words)\n",
    "    agg_wl_words.extend(wl_words)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words, coin, wl_appearance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0a87a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28871\n"
     ]
    }
   ],
   "source": [
    "# The number of raw single word extracted from 20 whitepapers\n",
    "print(len(agg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82f220b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered single word from 20 whitepapers\n",
    "print(len(agg_wl_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2440b",
   "metadata": {},
   "source": [
    "## Two Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faf8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_2gram = []\n",
    "agg_wl_words_2 = []\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_2, deduped_appearance_dict_2 = dedupe(list(ngrams(words_ls, 2)), words_context_dict)\n",
    "    raw_words_2gram, wl_words_2, wl_appearance_dict_2 = filter_wl_bl_words(deduped_words_2, deduped_appearance_dict_2, wl, bl)\n",
    "    agg_words_2gram.extend(raw_words_2gram)\n",
    "    agg_wl_words_2.extend(wl_words_2)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_2, coin, wl_appearance_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4810b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95618\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 2-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_2gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "569e917a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 2-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55b47d",
   "metadata": {},
   "source": [
    "## Three Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c455cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_3gram = []\n",
    "agg_wl_words_3 = []\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_3, deduped_appearance_dict_3 = dedupe(list(ngrams(words_ls, 3)), words_context_dict)\n",
    "    raw_words_3gram, wl_words_3, wl_appearance_dict_3 = filter_wl_bl_words(deduped_words_3, deduped_appearance_dict_3, wl, bl)\n",
    "    agg_words_3gram.extend(raw_words_3gram)\n",
    "    agg_wl_words_3.extend(wl_words_3)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_3, coin, wl_appearance_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beeefd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111328\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 3-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_3gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb27730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 3-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6802675",
   "metadata": {},
   "source": [
    "## Four Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad91b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_words_4gram = []\n",
    "agg_wl_words_4 = []\n",
    "for coin in words_bank:\n",
    "    words_ls = words_bank[coin]\n",
    "    words_context_dict = context_ref[coin]\n",
    "    deduped_words_4, deduped_appearance_dict_4 = dedupe(list(ngrams(words_ls, 4)), words_context_dict)\n",
    "    raw_words_4gram, wl_words_4, wl_appearance_dict_4 = filter_wl_bl_words(deduped_words_4, deduped_appearance_dict_4, wl, bl)\n",
    "    agg_words_4gram.extend(raw_words_4gram)\n",
    "    agg_wl_words_4.extend(wl_words_4)\n",
    "    enrich_lookup_dict(lookup_dict, wl_words_4, coin, wl_appearance_dict_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e88cd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114807\n"
     ]
    }
   ],
   "source": [
    "# The number of raw 4-gram extracted from 20 whitepapers\n",
    "print(len(agg_words_4gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b73237f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "# The number of filtered 4-gram from 20 whitepapers\n",
    "print(len(agg_wl_words_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6858865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
