{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd8d5a2",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73538a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406767f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install BeautifulSoup4\n",
    "# import nltk\n",
    "# nltk.download()  # Download text data sets, including stop words\n",
    "# !pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3181b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.corpus import words\n",
    "from stop_words import get_stop_words\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98f1fe0-dcf2-4e47-8fb0-726bd1f9c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a587e8c-d2d7-4f9a-8d5f-e18ef62e8de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'require'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"requires\", pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f88992",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Whitepaper datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c185377-0b77-4feb-9f78-8cb770062e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12a9f03-2ef8-4acf-800e-198377ecba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean(line, stopwords):\n",
    "    # words = [x.strip() for x in re.split(',| |\\. |\\: ', line) if x]\n",
    "    # words = map(str.lower, words)\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', line.lower()))\n",
    "    # words = [x.replace('-', '') for x in words]\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    words = [lemmatizer.lemmatize(w, pos='s') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='n') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "    words = [lemmatizer.lemmatize(w, pos='a') for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c198f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_whitepapers(filename, stopwords):\n",
    "    directory = \"../whitepapers/top20_whitepapers/\"\n",
    "    words_list = []\n",
    "    words_context_dict = {}\n",
    "    # context_tuples_ref = []\n",
    "    word_idx = 0\n",
    "    context_idx = 0\n",
    "    for entry in os.scandir(directory):\n",
    "        if (entry.path.endswith(filename) and entry.is_file()):\n",
    "            with open(entry.path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    # context_tuples_ref += [line]\n",
    "                    temp_words = extract_and_clean(line, stopwords)\n",
    "                    words_list.extend(temp_words)\n",
    "                    for i in range(len(temp_words)):\n",
    "                        words_context_dict[word_idx] = context_idx\n",
    "                        word_idx += 1\n",
    "                    context_idx += 1\n",
    "    return words_list, words_context_dict #, context_tuples_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8584fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['Algorand.txt', 'Avalanche.txt', 'Binance.txt', 'Bitcoin.txt', 'Cardano.txt', 'Chainlink.txt',\n",
    "            'Crypto_com.txt', 'Ethereum.txt', 'FTX_token.txt', 'PolkaDot.txt', 'Polygon.txt', 'Ripple.txt', \n",
    "            'Solana.txt', 'Terra.txt', 'Tether.txt', 'Tron.txt', 'Uniswap.txt', 'Wrapped.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbb718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_word_bank():\n",
    "    word_bank = {}\n",
    "    context_ref = {}\n",
    "    for i in filenames:\n",
    "        word_bank[i], context_ref[i] = read_whitepapers(i, stopwords_set)\n",
    "    return word_bank, context_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44cd5e47-e88f-4dc8-8342-3189631e774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(words, context_ref):\n",
    "    appearance_dict = {}\n",
    "    for i in range(len(words)):\n",
    "        appearance_dict.setdefault(words[i],[]).append(context_ref[i])\n",
    "    return list(set(words)), appearance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9df150-7ac5-4373-92d1-cdf7d37705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bank, context_ref = creat_word_bank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d17207-3680-40eb-89e9-a8e187942af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20eed08e-4138-45e4-8ad1-769cb44dde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lookup_dict(lookup_dict, words, paper_name, appearance_dict):\n",
    "    for w in words:\n",
    "        lookup_dict.setdefault(w,[]).append((paper_name, appearance_dict[w]))\n",
    "        # lookup_dict[w] = paper_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c428092-652c-48b1-ab61-93261ebc81c2",
   "metadata": {},
   "source": [
    "# Construct Word Counter for n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e39b53-df97-4a75-b4fa-1474dc4bc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single words\n",
    "agg_words = []\n",
    "for coin in word_bank:\n",
    "    deduped_words, appearance_dict = dedupe(word_bank[coin], context_ref[coin])\n",
    "    agg_words.extend(deduped_words)\n",
    "    enrich_lookup_dict(lookup_dict, deduped_words, coin, appearance_dict)\n",
    "\n",
    "single_counter = Counter(agg_words)\n",
    "# print(single_counter.most_common(300))\n",
    "# print([x[0] for x in single_counter.most_common(2000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ac97f2-998c-4e3c-bd5e-f4026e56a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-gram counter\n",
    "agg_words_2gram = []\n",
    "for coin in word_bank:\n",
    "    deduped_words, appearance_dict = dedupe(list(ngrams(word_bank[coin], 2)), context_ref[coin])\n",
    "    agg_words_2gram.extend(deduped_words)\n",
    "    enrich_lookup_dict(lookup_dict, deduped_words, coin, appearance_dict)\n",
    "\n",
    "two_gram_counter = Counter(agg_words_2gram)\n",
    "# print(two_gram_counter.most_common(250))\n",
    "# print([x[0] for x in two_gram_counter.most_common(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26076b59-2a9e-40c4-a290-a669fc7d340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-gram counter\n",
    "agg_words_3gram = []\n",
    "for coin in word_bank:\n",
    "    deduped_words, appearance_dict = dedupe(list(ngrams(word_bank[coin], 3)), context_ref[coin])\n",
    "    agg_words_3gram.extend(deduped_words)\n",
    "    enrich_lookup_dict(lookup_dict, deduped_words, coin, appearance_dict)\n",
    "\n",
    "three_gram_counter = Counter(agg_words_3gram)\n",
    "# print(three_gram_counter.most_common(150))\n",
    "# print([x[0] for x in three_gram_counter.most_common(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b3d30c-decd-41ea-a1d7-791c413e6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-gram counter\n",
    "agg_words_4gram = []\n",
    "for coin in word_bank:\n",
    "    deduped_words, appearance_dict = dedupe(list(ngrams(word_bank[coin], 4)), context_ref[coin])\n",
    "    agg_words_4gram.extend(deduped_words)\n",
    "    enrich_lookup_dict(lookup_dict, deduped_words, coin, appearance_dict)\n",
    "\n",
    "four_gram_counter = Counter(agg_words_4gram)\n",
    "# print(four_gram_counter.most_common(150))\n",
    "# print([x[0] for x in four_gram_counter.most_common(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d66fa-e137-4f42-bc79-be865594e03e",
   "metadata": {},
   "source": [
    "# Show the most common word/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a782ee-d2a1-4abf-83fd-1e64b1f7ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_common(counter, top_n, min_count, max_count):\n",
    "    print_count = 0\n",
    "    for pair in counter.most_common():\n",
    "        if print_count == top_n:\n",
    "            break\n",
    "        if pair[1] <= max_count and pair[1] >= min_count:\n",
    "            print(str(pair[0]) + \" ---  Count: \" + str(pair[1]))\n",
    "            print_count = print_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69e5ed9f-7fe1-4ea5-a1fc-0f96f4cce817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ---  Count: 15\n",
      "point ---  Count: 15\n",
      "model ---  Count: 15\n",
      "potential ---  Count: 15\n",
      "private ---  Count: 15\n",
      "would ---  Count: 15\n",
      "25 ---  Count: 15\n",
      "add ---  Count: 15\n",
      "update ---  Count: 15\n",
      "function ---  Count: 15\n"
     ]
    }
   ],
   "source": [
    "show_most_common(single_counter, 10, 2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3596e116-6fba-4587-8c1b-399618cbf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reference', '1') ---  Count: 10\n",
      "('1', 'introduction') ---  Count: 10\n",
      "('smart', 'contract') ---  Count: 10\n",
      "('private', 'key') ---  Count: 10\n",
      "('transaction', 'fee') ---  Count: 10\n",
      "('allow', 'user') ---  Count: 9\n",
      "('white', 'paper') ---  Count: 9\n",
      "('1', '2') ---  Count: 8\n",
      "('large', 'number') ---  Count: 8\n",
      "('decentralize', 'exchange') ---  Count: 8\n"
     ]
    }
   ],
   "source": [
    "show_most_common(two_gram_counter, 10, 2, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ccdc8da-b3d2-4c72-b9d4-ad3f97d0df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bitcoin.txt', [82]), ('Ethereum.txt', [119, 865]), ('Solana.txt', [97, 154, 154]), ('Tron.txt', [625, 626])]\n"
     ]
    }
   ],
   "source": [
    "print(lookup_dict['sha256'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6716d1b-0f1b-4e7b-b030-04815e71e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Avalanche.txt', [280, 307]), ('Bitcoin.txt', [10, 11, 17, 79, 82, 85, 87, 93, 95, 96, 99, 103, 109, 110, 163, 168, 327, 332]), ('Cardano.txt', [3146]), ('Chainlink.txt', [3197, 5249, 5251]), ('Ethereum.txt', [40, 41, 103, 116, 146, 171, 177, 438, 674]), ('PolkaDot.txt', [76])]\n"
     ]
    }
   ],
   "source": [
    "print(lookup_dict['proofofwork'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03fcd5bb-8914-437c-a2fc-9b42f43c98ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Avalanche.txt', [139, 280, 311]), ('Cardano.txt', [0, 3479, 3534]), ('Chainlink.txt', [652, 3198, 3285, 4377, 5249, 5251]), ('Ethereum.txt', [49, 855]), ('PolkaDot.txt', [407, 1467, 1467, 1474]), ('Polygon.txt', [59])]\n"
     ]
    }
   ],
   "source": [
    "print(lookup_dict['proofofstake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc985ed-aef5-479f-ab79-5daf5a3a95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Avalanche.txt', [41, 79, 87, 96, 125, 180, 191, 275, 289, 292, 295, 517, 555]), ('Cardano.txt', [3280, 3282]), ('Chainlink.txt', [208, 375, 593, 631, 679, 732, 970, 981, 1551, 1919, 1996, 2947, 4045, 4822, 5279]), ('Ethereum.txt', [199, 218]), ('PolkaDot.txt', [1398]), ('Ripple.txt', [3, 5]), ('Solana.txt', [546, 687])]\n"
     ]
    }
   ],
   "source": [
    "print(lookup_dict[('consensus', 'protocol')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e42c796-6e9d-400c-9d03-d5329539976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_context(lookup_dict, words):\n",
    "    directory = \"../whitepapers/top20_whitepapers/\"\n",
    "    appearances = lookup_dict[words]\n",
    "    # For each white paper, read and print relevant contents\n",
    "    for appear in appearances:\n",
    "        filename = appear[0]\n",
    "        idxs = appear[1]\n",
    "        context_idx = 0\n",
    "        for entry in os.scandir(directory):\n",
    "            if (entry.path.endswith(filename) and entry.is_file()):\n",
    "                with open(entry.path, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        if context_idx in idxs:\n",
    "                            print(filename + \" [line \" + str(context_idx) + \"] : \" + line)\n",
    "                        context_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2769372-3383-4cf6-a730-491aad670859",
   "metadata": {},
   "source": [
    "# Look up word/words appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27373135-d697-422a-8ba1-078a5e25d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalanche.txt [line 41] : Secure Avalanche is designed to be robust and achieve high security. Classical consensus protocols are\n",
      "\n",
      "Avalanche.txt [line 79] : of the platform is called “$AVAX”. The family of consensus protocols used by the Avalanche platform is\n",
      "\n",
      "Avalanche.txt [line 87] : of machines. Therefore, consensus protocols, which enable a group of nodes to achieve agreement, lie at the\n",
      "\n",
      "Avalanche.txt [line 96] : static deployments. Nakamoto consensus protocols [5,7,4], on the other hand, are robust, but suﬀer from\n",
      "\n",
      "Avalanche.txt [line 125] : consensus protocols and therefore require full membership knowledge. Knowing the entire set of par-\n",
      "\n",
      "Avalanche.txt [line 180] : of consensus protocols through a set of 8 critical axes.\n",
      "\n",
      "Avalanche.txt [line 191] : Table 1. Comparative chart between the three known families of consensus protocols. Avalanche, Snowman, and\n",
      "\n",
      "Avalanche.txt [line 275] : Consensus protocols provide their security guarantees under the assumption that up to a threshold number\n",
      "\n",
      "Avalanche.txt [line 289] : proposing participant. Unfortunately, there has also been substantial confusion between consensus protocols\n",
      "\n",
      "Avalanche.txt [line 292] : versus Sybil control mechanisms. We note that the choice of consensus protocols is, for the most part,\n",
      "\n",
      "Avalanche.txt [line 295] : guarantees of the consensus protocol. However, the Snow* family can be coupled with many of these known\n",
      "\n",
      "Avalanche.txt [line 517] : simple example is that of leader-based consensus protocols, wherein a subcommittee or a designated leader\n",
      "\n",
      "Avalanche.txt [line 555] : 6. Rocket, T.: Snowﬂake to Avalanche: A novel metastable consensus protocol family for cryptocurrencies. IPFS\n",
      "\n",
      "Cardano.txt [line 3280] : be be less eﬀective against a PoS based consensus protocol than a PoW based one. Currently our\n",
      "\n",
      "Cardano.txt [line 3282] : against PoS based consensus protocols is left as a future work.\n",
      "\n",
      "Chainlink.txt [line 208] : maintained by a committee of Chainlink nodes. Rooted in a consensus protocol, it\n",
      "\n",
      "Chainlink.txt [line 375] : based or permissionless consensus protocols, which they combine with the blockchains\n",
      "\n",
      "Chainlink.txt [line 593] : consensus protocol and run by a set of oracle nodes. A DON is designed primarily\n",
      "\n",
      "Chainlink.txt [line 631] : emphasize that DONs can be realized using permissionless consensus protocols.\n",
      "\n",
      "Chainlink.txt [line 679] : written to L even if the consensus protocol used for L doesn’t employ them.\n",
      "\n",
      "Chainlink.txt [line 732] : permissionless consensus protocol and DON deployers may ultimately choose to adopt\n",
      "\n",
      "Chainlink.txt [line 970] : a complete BFT consensus protocol. Nodes do not maintain message logs that are\n",
      "\n",
      "Chainlink.txt [line 981] : OCR to be a full-blown consensus protocol, they do require OCR to provide some addi-\n",
      "\n",
      "Chainlink.txt [line 1551] : The problem of undue inﬂuence by leaders on transaction ordering in consensus\n",
      "\n",
      "Chainlink.txt [line 1919] : on all transactions is the very problem solved by the consensus protocol underlying\n",
      "\n",
      "Chainlink.txt [line 1996] : ﬁned by Cachin et al. [71], where it was integrated with a permissioned consensus\n",
      "\n",
      "Chainlink.txt [line 2947] : ing or adversarially ordering transactions. The underlying consensus protocol in a\n",
      "\n",
      "Chainlink.txt [line 4045] : [72] Cachin, C., and Vukolic´, M. Blockchain consensus protocols in the wild. arXiv preprint\n",
      "\n",
      "Chainlink.txt [line 4822] : consensus protocol). The other three can only achieve this property by having an\n",
      "\n",
      "Chainlink.txt [line 5279] : This observation underlies many hybrid consensus protocols, e.g., [120, 182] as well\n",
      "\n",
      "Ethereum.txt [line 199] : for the Bitcoin consensus protocol. Namecoin is the oldest, and most successful, implementation of a\n",
      "\n",
      "Ethereum.txt [line 218] : Thus, in general, there are two approaches toward building a consensus protocol: building an independent\n",
      "\n",
      "PolkaDot.txt [line 1398] : the consensus protocol.\n",
      "\n",
      "Ripple.txt [line 3] : This paper does not reflect the current state of the ledger consensus protocol or its \n",
      "\n",
      "Ripple.txt [line 5] : used as a reference. For an updated analysis and presentation of the consensus \n",
      "\n",
      "Solana.txt [line 546] : The consensus protocol provides a second layer of defense, as any attack\n",
      "\n",
      "Solana.txt [line 687] : To limit this attack, the consensus protocol chosen for the network can\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup_context(lookup_dict, ('consensus', 'protocol'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "762f5646-6782-4d53-bcb8-06194ea342c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin.txt [line 82] : The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the \n",
      "\n",
      "Ethereum.txt [line 119] : preventing sybil attackers from remaking the entire blockchain in their favor. Because SHA256 is designed\n",
      "\n",
      "Ethereum.txt [line 865] : The Bitcoin mining algorithm works by having miners compute SHA256 on slightly modified versions of\n",
      "\n",
      "Solana.txt [line 97] : sha256, ripemd, etc.), run the function from some random starting value\n",
      "\n",
      "Solana.txt [line 154] : sha256 of the photograph. The index and the sha256 of the photograph are\n",
      "\n",
      "Tron.txt [line 625] : The raw data then undergoes SHA-256 hashing. The private key corresponding to the contract \n",
      "\n",
      "Tron.txt [line 626] : address then signs the result of the SHA256 hash. The signature result is then added to the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup_context(lookup_dict, 'sha256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fcf4288-4610-4b6f-96f6-12c96dcdfe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardano.txt [line 4] : We present “Ouroboros,” the ﬁrst blockchain protocol based on proof of stake with rig-\n",
      "\n",
      "Cardano.txt [line 6] : those achieved by the bitcoin blockchain protocol. As the protocol provides a “proof of stake”\n",
      "\n",
      "Cardano.txt [line 9] : centivizing proof of stake protocols and we prove that, given this mechanism, honest behavior\n",
      "\n",
      "Cardano.txt [line 27] : A natural alternative mechanism relies on the notion of “proof of stake” (PoS). Rather than\n",
      "\n",
      "Cardano.txt [line 43] : ideal; however, realizing such a proof of stake protocol appears to involve a number of deﬁnitional,\n",
      "\n",
      "Cardano.txt [line 45] : Previous work. The concept of PoS has been discussed extensively in the bitcoin forum.1 Proof\n",
      "\n",
      "Cardano.txt [line 50] : Heuristic proof of stake based blockchain protocols have been proposed (and implemented) for a\n",
      "\n",
      "Cardano.txt [line 73] : Our Results. We present “Ouroboros,” a provably secure proof of stake system. To the best of\n",
      "\n",
      "Cardano.txt [line 80] : 1See “Proof of stake instead of proof of work”, Bitcoin forum thread. Posts by user “QuantumMechanic” and\n",
      "\n",
      "Cardano.txt [line 148] : report on benchmark experiments run in the Amazon cloud that showcase the power of our proof\n",
      "\n",
      "Cardano.txt [line 3488] : bitcoin’s proof of work via proof of stake [extended abstract]y. SIGMETRICS Performance\n",
      "\n",
      "Cardano.txt [line 3492] : [9] Iddo Bentov, Rafael Pass, and Elaine Shi. Snow white: Provably secure proofs of stake. IACR\n",
      "\n",
      "Cardano.txt [line 3512] : [15] Vitalik Buterin. Proof of stake faq. https://github.com/ethereum/wiki/wiki/Proof-of-Stake-\n",
      "\n",
      "Polygon.txt [line 65] : a selected set of Block Producers, chosen for every checkpoint by a set of Stakers. It then uses a Proof Of\n",
      "\n",
      "Polygon.txt [line 172] : The Matic Network uses a dual strategy of Proof of Stake at the checkpointing layer and Block Producers\n",
      "\n",
      "Solana.txt [line 22] : consensus algorithm such as Proof of Work (PoW) or Proof of Stake\n",
      "\n",
      "Solana.txt [line 49] : in Section 4. In depth description of the proposed Proof of Stake consensus\n",
      "\n",
      "Solana.txt [line 348] : 5 Proof of Stake Consensus\n",
      "\n",
      "Solana.txt [line 350] : This specific instance of Proof of Stake is designed for quick confirmation of\n",
      "\n",
      "Solana.txt [line 361] : slashing The proposed solution to the nothing at stake problem in Proof\n",
      "\n",
      "Solana.txt [line 456] : Proof of Stake verifiers lock up some amount of coin in a “stake”, which\n",
      "\n",
      "Solana.txt [line 720] : The Proof of Stake bond’s table contains:\n",
      "\n",
      "Solana.txt [line 731] : The network could be configured with a minimum Proof of Stake bond\n",
      "\n",
      "Solana.txt [line 840] : [7] Slasher, A punative Proof of Stake algorithm\n",
      "\n",
      "Solana.txt [line 843] : [8] BitShares Delegated Proof of Stake\n",
      "\n",
      "Terra.txt [line 143] : The Terra Protocol runs on a Proof of Stake (PoS) blockchain, where miners need to stake a native\n",
      "\n",
      "Tron.txt [line 50] : 3.1 Delegated Proof of Stake (DPoS) 13 \n",
      "\n",
      "Tron.txt [line 64] : 5.2.4 Transaction as Proof of Stake (TaPoS) 20 \n",
      "\n",
      "Tron.txt [line 226] : network. See 3.1 Delegated Proof of Stake (DPoS) for more details. \n",
      "\n",
      "Tron.txt [line 282] : mechanism is based on Delegated Proof of Stake (DPoS) and many innovations were made in \n",
      "\n",
      "Tron.txt [line 358] : 3.1 Delegated Proof of Stake (DPoS) \n",
      "\n",
      "Tron.txt [line 424] : To solve the energy waste issue, the Proof of Stake (PoS) consensus mechanism was proposed by \n",
      "\n",
      "Tron.txt [line 431] : The TRON consensus mechanism uses an innovative Delegated Proof of Stake system in which 27 \n",
      "\n",
      "Tron.txt [line 676] : 5.2.4 Transaction as Proof of Stake (TaPoS) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup_context(lookup_dict, ('proof', 'stake'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72215cc3-21e0-4326-baa5-a1aac27595f2",
   "metadata": {},
   "source": [
    "# Construct Whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc8c280e-0c2c-4f2b-89b0-fccd3cff1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_word_count(orig, min_c, max_c):\n",
    "    result = []\n",
    "    for pair in orig:\n",
    "        if pair[1] >= min_c and pair[1] <= max_c:\n",
    "            result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "653c68ee-850c-45b4-9b68-99e4bd836836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2-gram, 3-gram, and 4-gram\n",
    "def filter_numeric_or_short_words(orig):\n",
    "    result = []\n",
    "    for pair in orig:\n",
    "        has_num = False\n",
    "        for w in pair[0]:\n",
    "            if w.isnumeric() or len(w) < 3:\n",
    "                has_num = True\n",
    "                break\n",
    "        if not has_num:\n",
    "            result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d6634ae-3570-429d-ac05-f5735bd33484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2-gram, 3-gram, and 4-gram\n",
    "def filter_aggresive_p1(orig):\n",
    "    result = []\n",
    "    modals = ['can', 'could', 'may', 'might', 'must', 'will', 'would', 'should']\n",
    "    stop_words = list(get_stop_words('en'))         #Have around 900 stopwords\n",
    "    nltk_words = list(stopwords.words('english'))   #Have around 150 stopwords\n",
    "    stop_words.extend(nltk_words)\n",
    "    corpus = set(words.words())\n",
    "    for pair in orig:\n",
    "        include_ = True\n",
    "        prev_ = \"\"\n",
    "        for w in pair[0]:\n",
    "            # Filter Modal Verb words\n",
    "            if w in modals:\n",
    "                include_ = False\n",
    "                break\n",
    "            # Filter stop words\n",
    "            if w in stop_words:\n",
    "                include_ = False\n",
    "                break\n",
    "            # Filter cid words\n",
    "            if \"cid\" in w:\n",
    "                include_ = False\n",
    "                break\n",
    "            # Filter user related words\n",
    "            if w == 'user':\n",
    "                include_ = False\n",
    "                break\n",
    "            # Filter user related words\n",
    "            if w == 'section':\n",
    "                include_ = False\n",
    "                break\n",
    "            # Filter repetitive words:\n",
    "            if w == prev_:\n",
    "                include_ = False\n",
    "                break\n",
    "            if len(w) > 16 and w not in corpus:\n",
    "                include_ = False\n",
    "                break\n",
    "            prev_ = w\n",
    "        if include_:\n",
    "            result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64af9cca-0f75-4652-b5e9-d77b79423f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2-gram, 3-gram, and 4-gram\n",
    "def filter_aggresive_p2(orig):\n",
    "    result = []\n",
    "    for pair in orig:\n",
    "        include_ = True\n",
    "        for w in pair[0]:\n",
    "            # NLTK classify the type of the word\n",
    "            type_ = pos_tag([w])[0][1]\n",
    "            \n",
    "            # https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "            \n",
    "            # then filter preposition/subordinating conjunction\n",
    "            if type_ == 'IN':\n",
    "                include_ = False\n",
    "                break\n",
    "            # filter conjunction\n",
    "            if type_ == 'CONJ':\n",
    "                include_ = False\n",
    "                break\n",
    "            # filter non-comparative and non-superlative adverbs\n",
    "            if type_ == 'RB':\n",
    "                include_ = False\n",
    "                break\n",
    "        if include_:\n",
    "            result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70aa9de1-83c9-461f-939d-2dd07b92ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 4-gram\n",
    "def filter_aggresive_p3(orig):\n",
    "    result = []\n",
    "    for pair in orig:\n",
    "        # Example: merkle,root,merkle,root\n",
    "        # Example: block,header,block,header\n",
    "        if pair[0][0] == pair[0][2] and pair[0][1] == pair[0][3]:\n",
    "            continue\n",
    "        result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb1e0e63-fc56-4b04-ac5c-31cf4365b2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RB'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"also\"])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f9dcc55-fc43-4885-a8a1-c7ffc71baeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_numeric_or_short_word(orig):\n",
    "    result = []\n",
    "    for pair in orig:\n",
    "        if not pair[0].isnumeric() and len(pair[0]) > 3:\n",
    "            result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5eff311-c90b-49a5-be7c-8df5dbcabf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_with_filter_singleword(arr, min_c, max_c, outfile):\n",
    "    with open(outfile, 'w') as f:\n",
    "        for pair in arr:\n",
    "            if pair[1] >= min_c and pair[1] <= max_c:\n",
    "                print(pair[0], file=f)\n",
    "    print(\"Output to filename \" + outfile + \" - [Done]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2871ecf-82ff-49ef-89ca-66213779c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_with_filter(arr, min_c, max_c, outfile):\n",
    "    with open(outfile, 'w') as f:\n",
    "        for pair in arr:\n",
    "            if pair[1] >= min_c and pair[1] <= max_c:\n",
    "                print(','.join(pair[0]), file=f)\n",
    "                # print(pair[0], file=f)\n",
    "    print(\"Output to filename \" + outfile + \" - [Done]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd60ba13-f6c0-4bc2-953e-10261681f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to filename whitelist/word2_w.txt - [Done]\n"
     ]
    }
   ],
   "source": [
    "filtered_words_ls = filter_numeric_or_short_words(two_gram_counter.most_common())\n",
    "filtered_words_ls = filter_word_count(filtered_words_ls, 2, 18)\n",
    "filtered_words_ls = filter_aggresive_p1(filtered_words_ls)\n",
    "filtered_words_ls = filter_aggresive_p2(filtered_words_ls)\n",
    "print_results_with_filter(filtered_words_ls, 2, 18, \"whitelist/word2_w.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b48032d-5b98-4c73-9b14-3cda80661331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to filename whitelist/word3_w.txt - [Done]\n"
     ]
    }
   ],
   "source": [
    "filtered_words_ls = filter_numeric_or_short_words(three_gram_counter.most_common())\n",
    "filtered_words_ls = filter_word_count(filtered_words_ls, 2, 18)\n",
    "filtered_words_ls = filter_aggresive_p1(filtered_words_ls)\n",
    "filtered_words_ls = filter_aggresive_p2(filtered_words_ls)\n",
    "print_results_with_filter(filtered_words_ls, 2, 18, \"whitelist/word3_w.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bee98d2e-2f3c-48cf-ab80-d02f2fea41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to filename whitelist/word4_w.txt - [Done]\n"
     ]
    }
   ],
   "source": [
    "filtered_words_ls = filter_numeric_or_short_words(four_gram_counter.most_common())\n",
    "filtered_words_ls = filter_word_count(filtered_words_ls, 2, 18)\n",
    "filtered_words_ls = filter_aggresive_p1(filtered_words_ls)\n",
    "filtered_words_ls = filter_aggresive_p2(filtered_words_ls)\n",
    "filtered_words_ls = filter_aggresive_p3(filtered_words_ls)\n",
    "print_results_with_filter(filtered_words_ls, 2, 18, \"whitelist/word4_w.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16048caa-cc28-4b23-a7f8-4800149db537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_results_with_filter_singleword(filter_numeric_or_short_word(single_counter.most_common()), 2, 12, \"whitelist/word1_w.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3da84076-c5b6-432e-ae35-f5417cbc2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single word only\n",
    "def filter_aggresive_p4(orig):\n",
    "    result = []\n",
    "    stop_words = list(get_stop_words('en'))         #Have around 900 stopwords\n",
    "    nltk_words = list(stopwords.words('english'))   #Have around 150 stopwords\n",
    "    stop_words.extend(nltk_words)\n",
    "    corpus = set(words.words())\n",
    "    for pair in orig:\n",
    "        w = pair[0]\n",
    "        # Filter Modal Verb words\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        if w in corpus:\n",
    "            continue\n",
    "        result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c805771b-81da-48a8-96ad-cab950b19038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to filename whitelist/word1_w.txt - [Done]\n"
     ]
    }
   ],
   "source": [
    "filtered_words_ls = filter_numeric_or_short_word(single_counter.most_common())\n",
    "filtered_words_ls = filter_word_count(filtered_words_ls, 2, 16)\n",
    "filtered_words_ls = filter_aggresive_p4(filtered_words_ls)\n",
    "print_results_with_filter_singleword(filtered_words_ls, 3, 16, \"whitelist/word1_w.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818533e-b2af-480f-84e8-ad0e8e714633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1166e6-0412-4cff-8581-a2acc96d958d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
